{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cross Validation "
      ],
      "metadata": {
        "id": "7OuDd7XNJ_Ne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a notebook that introduces the training of eight models of the Cross-Validation method."
      ],
      "metadata": {
        "id": "AZhMbxm2M1lJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "50JtgUVW7RzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connecting to google drive to save the models there."
      ],
      "metadata": {
        "id": "l1ZPGAxW7XsY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5w9IQRsk_ZM1",
        "outputId": "d08d5f6d-c2ce-4ab9-bd53-f5c7599090b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cloning RE-BERT repository."
      ],
      "metadata": {
        "id": "-7G9Hg_y7ix7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_rBUwWstoT-",
        "outputId": "e32b35fb-264a-4a86-d0b6-c8b07600d443"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RE-BERT'...\n",
            "remote: Enumerating objects: 197, done.\u001b[K\n",
            "remote: Counting objects: 100% (111/111), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 197 (delta 64), reused 81 (delta 42), pack-reused 86\u001b[K\n",
            "Receiving objects: 100% (197/197), 3.02 MiB | 20.75 MiB/s, done.\n",
            "Resolving deltas: 100% (91/91), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/BruNamie/RE-BERT\n",
        "!mv RE-BERT/* ."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing dependencies."
      ],
      "metadata": {
        "id": "HhmHKI7l7lF4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTqQLZOitxNf",
        "outputId": "b8ff0f9c-bffc-4214-b792-62e914c648a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.21.6)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.12.1+cu113)\n",
            "Collecting transformers==2.3.0\n",
            "  Downloading transformers-2.3.0-py3-none-any.whl (447 kB)\n",
            "\u001b[K     |████████████████████████████████| 447 kB 33.5 MB/s \n",
            "\u001b[?25hCollecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from transformers==2.3.0->-r requirements.txt (line 3)) (4.64.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 49.4 MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.26.4-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 59.7 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 57.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.3.0->-r requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.3.0->-r requirements.txt (line 3)) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.0->-r requirements.txt (line 2)) (4.1.1)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.3 MB/s \n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.4\n",
            "  Downloading botocore-1.29.4-py3-none-any.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 57.3 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 74.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.4->boto3->transformers==2.3.0->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.4->boto3->transformers==2.3.0->-r requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.3.0->-r requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.3.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.3.0->-r requirements.txt (line 3)) (2022.9.24)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 51.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.3.0->-r requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.3.0->-r requirements.txt (line 3)) (1.2.0)\n",
            "Building wheels for collected packages: sklearn, sacremoses\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=3b0fe73fb0f0e7c337565e5ba5d1608d8ef548bd923a027d0bea844551ed01ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/56/cc/4a8bf86613aafd5b7f1b310477667c1fca5c51c3ae4124a003\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=b3d0685e6aa722c722aa2c77d1f4bbe7b3e54c659854ff981c24754a1a0d9380\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sklearn sacremoses\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, sentencepiece, sacremoses, boto3, transformers, sklearn\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed boto3-1.26.4 botocore-1.29.4 jmespath-1.0.1 s3transfer-0.6.0 sacremoses-0.0.53 sentencepiece-0.1.97 sklearn-0.0.post1 transformers-2.3.0 urllib3-1.25.11\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ny3QI4KIqiP"
      },
      "source": [
        "# Cross Validation "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Cross-Validation performed below will be done according to the article \"*RE-BERT: Automatic Extraction of Software Requirements from App Reviews using BERT Language Model*\" by Adailton Ferreira de Araújo and Ricardo Marcondes Marcacini."
      ],
      "metadata": {
        "id": "p4BmMTXw9F5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The image below, taken from the article,  shows this strategy for a scenario involving reviews from 8 different apps: ![imagem_2022-10-25_180514291.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "MZUK4CNt9emx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to note that the codes presented below are repeated for each iteration. The training was carried out in this way due to limitations of Google Colab, the platform used for this. "
      ],
      "metadata": {
        "id": "fAvUUWszP1D5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SiclfWBtnBZ"
      },
      "source": [
        "Iteration #1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUc_LrxrtyhG",
        "outputId": "320a0c56-7c65-42ff-fabc-64153d2eafe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "INFO:transformers.configuration_utils:Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "INFO:root:cuda memory allocated: 455604736\n",
            "cuda memory allocated: 455604736\n",
            "INFO:root:> n_trainable_params: 113617923, n_nontrainable_params: 0\n",
            "> n_trainable_params: 113617923, n_nontrainable_params: 0\n",
            "INFO:root:> training arguments:\n",
            "> training arguments:\n",
            "INFO:root:>>> model_name: RE_BERT\n",
            ">>> model_name: RE_BERT\n",
            "INFO:root:>>> dataset: CV0\n",
            ">>> dataset: CV0\n",
            "INFO:root:>>> train_file: train_iob_eBay.txt\n",
            ">>> train_file: train_iob_eBay.txt\n",
            "INFO:root:>>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            "INFO:root:>>> initializer: <function xavier_uniform_ at 0x7f116be65170>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f116be65170>\n",
            "INFO:root:>>> lr: 2e-05\n",
            ">>> lr: 2e-05\n",
            "INFO:root:>>> dropout: 0.1\n",
            ">>> dropout: 0.1\n",
            "INFO:root:>>> l2reg: 0.01\n",
            ">>> l2reg: 0.01\n",
            "INFO:root:>>> num_epoch: 1\n",
            ">>> num_epoch: 1\n",
            "INFO:root:>>> batch_size: 16\n",
            ">>> batch_size: 16\n",
            "INFO:root:>>> log_step: 10\n",
            ">>> log_step: 10\n",
            "INFO:root:>>> bert_dim: 768\n",
            ">>> bert_dim: 768\n",
            "INFO:root:>>> pretrained_bert_name: bert-base-uncased\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            "INFO:root:>>> max_seq_len: 80\n",
            ">>> max_seq_len: 80\n",
            "INFO:root:>>> requirement_dim: 3\n",
            ">>> requirement_dim: 3\n",
            "INFO:root:>>> hops: 3\n",
            ">>> hops: 3\n",
            "INFO:root:>>> patience: 5\n",
            ">>> patience: 5\n",
            "INFO:root:>>> device: cuda\n",
            ">>> device: cuda\n",
            "INFO:root:>>> seed: None\n",
            ">>> seed: None\n",
            "INFO:root:>>> valset_ratio: 0\n",
            ">>> valset_ratio: 0\n",
            "INFO:root:>>> local_context_focus: cdm\n",
            ">>> local_context_focus: cdm\n",
            "INFO:root:>>> alpha: 3\n",
            ">>> alpha: 3\n",
            "INFO:root:>>> SRD: 3\n",
            ">>> SRD: 3\n",
            "INFO:root:>>> model_class: <class 'models.re_bert.RE_BERT'>\n",
            ">>> model_class: <class 'models.re_bert.RE_BERT'>\n",
            "INFO:root:>>> dataset_file: {'train': 'train_iob_eBay.txt'}\n",
            ">>> dataset_file: {'train': 'train_iob_eBay.txt'}\n",
            "INFO:root:>>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices', 'text_bert_indices', 'aspect_bert_indices']\n",
            ">>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices', 'text_bert_indices', 'aspect_bert_indices']\n",
            "INFO:root:>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "INFO:root:epoch: 1\n",
            "epoch: 1\n",
            "INFO:root:loss: 0.9486\n",
            "loss: 0.9486\n",
            "INFO:root:loss: 0.8916\n",
            "loss: 0.8916\n",
            "INFO:root:loss: 0.8836\n",
            "loss: 0.8836\n",
            "INFO:root:loss: 0.8632\n",
            "loss: 0.8632\n",
            "INFO:root:loss: 0.8441\n",
            "loss: 0.8441\n",
            "INFO:root:loss: 0.8363\n",
            "loss: 0.8363\n",
            "INFO:root:loss: 0.8143\n",
            "loss: 0.8143\n",
            "INFO:root:loss: 0.8024\n",
            "loss: 0.8024\n",
            "INFO:root:loss: 0.8085\n",
            "loss: 0.8085\n",
            "INFO:root:loss: 0.8011\n",
            "loss: 0.8011\n",
            "INFO:root:loss: 0.7924\n",
            "loss: 0.7924\n",
            "INFO:root:loss: 0.7991\n",
            "loss: 0.7991\n",
            "INFO:root:loss: 0.7967\n",
            "loss: 0.7967\n",
            "INFO:root:loss: 0.7907\n",
            "loss: 0.7907\n",
            "INFO:root:loss: 0.7828\n",
            "loss: 0.7828\n",
            "INFO:root:loss: 0.7874\n",
            "loss: 0.7874\n",
            "INFO:root:loss: 0.7725\n",
            "loss: 0.7725\n",
            "INFO:root:loss: 0.7650\n",
            "loss: 0.7650\n",
            "INFO:root:loss: 0.7604\n",
            "loss: 0.7604\n",
            "INFO:root:loss: 0.7499\n",
            "loss: 0.7499\n",
            "INFO:root:loss: 0.7403\n",
            "loss: 0.7403\n",
            "INFO:root:loss: 0.7339\n",
            "loss: 0.7339\n",
            "INFO:root:loss: 0.7283\n",
            "loss: 0.7283\n",
            "INFO:root:loss: 0.7207\n",
            "loss: 0.7207\n",
            "INFO:root:loss: 0.7166\n",
            "loss: 0.7166\n",
            "INFO:root:loss: 0.7094\n",
            "loss: 0.7094\n",
            "INFO:root:loss: 0.6999\n",
            "loss: 0.6999\n",
            "INFO:root:loss: 0.6946\n",
            "loss: 0.6946\n",
            "INFO:root:loss: 0.6883\n",
            "loss: 0.6883\n",
            "INFO:root:loss: 0.6814\n",
            "loss: 0.6814\n",
            "INFO:root:loss: 0.6775\n",
            "loss: 0.6775\n",
            "INFO:root:loss: 0.6730\n",
            "loss: 0.6730\n",
            "INFO:root:loss: 0.6679\n",
            "loss: 0.6679\n",
            "INFO:root:loss: 0.6626\n",
            "loss: 0.6626\n",
            "INFO:root:loss: 0.6579\n",
            "loss: 0.6579\n",
            "INFO:root:loss: 0.6523\n",
            "loss: 0.6523\n",
            "INFO:root:loss: 0.6456\n",
            "loss: 0.6456\n",
            "INFO:root:loss: 0.6396\n",
            "loss: 0.6396\n",
            "INFO:root:loss: 0.6361\n",
            "loss: 0.6361\n",
            "INFO:root:loss: 0.6341\n",
            "loss: 0.6341\n",
            "INFO:root:loss: 0.6293\n",
            "loss: 0.6293\n",
            "INFO:root:loss: 0.6263\n",
            "loss: 0.6263\n",
            "INFO:root:loss: 0.6194\n",
            "loss: 0.6194\n",
            "INFO:root:loss: 0.6153\n",
            "loss: 0.6153\n",
            "INFO:root:loss: 0.6104\n",
            "loss: 0.6104\n",
            "INFO:root:loss: 0.6073\n",
            "loss: 0.6073\n",
            "INFO:root:loss: 0.6020\n",
            "loss: 0.6020\n",
            "INFO:root:loss: 0.5980\n",
            "loss: 0.5980\n",
            "INFO:root:loss: 0.5936\n",
            "loss: 0.5936\n",
            "INFO:root:loss: 0.5914\n",
            "loss: 0.5914\n",
            "INFO:root:loss: 0.5875\n",
            "loss: 0.5875\n",
            "INFO:root:loss: 0.5858\n",
            "loss: 0.5858\n",
            "INFO:root:loss: 0.5819\n",
            "loss: 0.5819\n",
            "INFO:root:loss: 0.5794\n",
            "loss: 0.5794\n",
            "INFO:root:loss: 0.5772\n",
            "loss: 0.5772\n",
            "INFO:root:loss: 0.5741\n",
            "loss: 0.5741\n",
            "INFO:root:loss: 0.5704\n",
            "loss: 0.5704\n",
            "INFO:root:loss: 0.5675\n",
            "loss: 0.5675\n",
            "INFO:root:loss: 0.5638\n",
            "loss: 0.5638\n",
            "INFO:root:loss: 0.5618\n",
            "loss: 0.5618\n",
            "INFO:root:loss: 0.5592\n",
            "loss: 0.5592\n",
            "INFO:root:loss: 0.5561\n",
            "loss: 0.5561\n",
            "INFO:root:loss: 0.5525\n",
            "loss: 0.5525\n",
            "INFO:root:loss: 0.5520\n",
            "loss: 0.5520\n",
            "INFO:root:loss: 0.5509\n",
            "loss: 0.5509\n",
            "INFO:root:loss: 0.5490\n",
            "loss: 0.5490\n",
            "INFO:root:loss: 0.5469\n",
            "loss: 0.5469\n",
            "INFO:root:loss: 0.5437\n",
            "loss: 0.5437\n",
            "INFO:root:loss: 0.5408\n",
            "loss: 0.5408\n",
            "INFO:root:loss: 0.5396\n",
            "loss: 0.5396\n",
            "INFO:root:loss: 0.5382\n",
            "loss: 0.5382\n",
            "INFO:root:loss: 0.5359\n",
            "loss: 0.5359\n",
            "INFO:root:loss: 0.5333\n",
            "loss: 0.5333\n",
            "INFO:root:loss: 0.5304\n",
            "loss: 0.5304\n",
            "INFO:root:loss: 0.5279\n",
            "loss: 0.5279\n",
            "INFO:root:loss: 0.5254\n",
            "loss: 0.5254\n",
            "INFO:root:loss: 0.5233\n",
            "loss: 0.5233\n",
            "INFO:root:loss: 0.5201\n",
            "loss: 0.5201\n",
            "INFO:root:loss: 0.5173\n",
            "loss: 0.5173\n",
            "INFO:root:loss: 0.5154\n",
            "loss: 0.5154\n",
            "INFO:root:loss: 0.5136\n",
            "loss: 0.5136\n",
            "INFO:root:loss: 0.5112\n",
            "loss: 0.5112\n",
            "INFO:root:loss: 0.5092\n",
            "loss: 0.5092\n",
            "INFO:root:>> saved: trained_models/RE_BERT_CV0_iob_epoch_1.model\n",
            ">> saved: trained_models/RE_BERT_CV0_iob_epoch_1.model\n"
          ]
        }
      ],
      "source": [
        "!python train.py --dataset CV0 --train_file train_iob_eBay.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hs3lSWuma_eg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "73486f13-d069-4295-da98-5bb1c182b151"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/Models/RE_BERT_CV0_iob_epoch_1.model'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "shutil.copy(\"trained_models/RE_BERT_CV0_iob_epoch_1.model\",\"/content/drive/MyDrive/Colab Notebooks/Models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEjIfqL2bFGO"
      },
      "source": [
        "Iteration #2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9upXwLGbGs3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7035c39a-c286-4cd8-f0cc-e8f9c8a900fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "INFO:transformers.configuration_utils:Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "INFO:root:cuda memory allocated: 455604736\n",
            "cuda memory allocated: 455604736\n",
            "INFO:root:> n_trainable_params: 113617923, n_nontrainable_params: 0\n",
            "> n_trainable_params: 113617923, n_nontrainable_params: 0\n",
            "INFO:root:> training arguments:\n",
            "> training arguments:\n",
            "INFO:root:>>> model_name: RE_BERT\n",
            ">>> model_name: RE_BERT\n",
            "INFO:root:>>> dataset: CV1\n",
            ">>> dataset: CV1\n",
            "INFO:root:>>> train_file: train_iob_Evernote.txt\n",
            ">>> train_file: train_iob_Evernote.txt\n",
            "INFO:root:>>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            "INFO:root:>>> initializer: <function xavier_uniform_ at 0x7ff245f37170>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7ff245f37170>\n",
            "INFO:root:>>> lr: 2e-05\n",
            ">>> lr: 2e-05\n",
            "INFO:root:>>> dropout: 0.1\n",
            ">>> dropout: 0.1\n",
            "INFO:root:>>> l2reg: 0.01\n",
            ">>> l2reg: 0.01\n",
            "INFO:root:>>> num_epoch: 1\n",
            ">>> num_epoch: 1\n",
            "INFO:root:>>> batch_size: 16\n",
            ">>> batch_size: 16\n",
            "INFO:root:>>> log_step: 10\n",
            ">>> log_step: 10\n",
            "INFO:root:>>> bert_dim: 768\n",
            ">>> bert_dim: 768\n",
            "INFO:root:>>> pretrained_bert_name: bert-base-uncased\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            "INFO:root:>>> max_seq_len: 80\n",
            ">>> max_seq_len: 80\n",
            "INFO:root:>>> requirement_dim: 3\n",
            ">>> requirement_dim: 3\n",
            "INFO:root:>>> hops: 3\n",
            ">>> hops: 3\n",
            "INFO:root:>>> patience: 5\n",
            ">>> patience: 5\n",
            "INFO:root:>>> device: cuda\n",
            ">>> device: cuda\n",
            "INFO:root:>>> seed: None\n",
            ">>> seed: None\n",
            "INFO:root:>>> valset_ratio: 0\n",
            ">>> valset_ratio: 0\n",
            "INFO:root:>>> local_context_focus: cdm\n",
            ">>> local_context_focus: cdm\n",
            "INFO:root:>>> alpha: 3\n",
            ">>> alpha: 3\n",
            "INFO:root:>>> SRD: 3\n",
            ">>> SRD: 3\n",
            "INFO:root:>>> model_class: <class 'models.re_bert.RE_BERT'>\n",
            ">>> model_class: <class 'models.re_bert.RE_BERT'>\n",
            "INFO:root:>>> dataset_file: {'train': 'train_iob_Evernote.txt'}\n",
            ">>> dataset_file: {'train': 'train_iob_Evernote.txt'}\n",
            "INFO:root:>>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices', 'text_bert_indices', 'aspect_bert_indices']\n",
            ">>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices', 'text_bert_indices', 'aspect_bert_indices']\n",
            "INFO:root:>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "INFO:root:epoch: 1\n",
            "epoch: 1\n",
            "INFO:root:loss: 0.8961\n",
            "loss: 0.8961\n",
            "INFO:root:loss: 0.8199\n",
            "loss: 0.8199\n",
            "INFO:root:loss: 0.8026\n",
            "loss: 0.8026\n",
            "INFO:root:loss: 0.8120\n",
            "loss: 0.8120\n",
            "INFO:root:loss: 0.8072\n",
            "loss: 0.8072\n",
            "INFO:root:loss: 0.7838\n",
            "loss: 0.7838\n",
            "INFO:root:loss: 0.7723\n",
            "loss: 0.7723\n",
            "INFO:root:loss: 0.7697\n",
            "loss: 0.7697\n",
            "INFO:root:loss: 0.7751\n",
            "loss: 0.7751\n",
            "INFO:root:loss: 0.7745\n",
            "loss: 0.7745\n",
            "INFO:root:loss: 0.7721\n",
            "loss: 0.7721\n",
            "INFO:root:loss: 0.7673\n",
            "loss: 0.7673\n",
            "INFO:root:loss: 0.7678\n",
            "loss: 0.7678\n",
            "INFO:root:loss: 0.7632\n",
            "loss: 0.7632\n",
            "INFO:root:loss: 0.7628\n",
            "loss: 0.7628\n",
            "INFO:root:loss: 0.7594\n",
            "loss: 0.7594\n",
            "INFO:root:loss: 0.7522\n",
            "loss: 0.7522\n",
            "INFO:root:loss: 0.7482\n",
            "loss: 0.7482\n",
            "INFO:root:loss: 0.7454\n",
            "loss: 0.7454\n",
            "INFO:root:loss: 0.7372\n",
            "loss: 0.7372\n",
            "INFO:root:loss: 0.7251\n",
            "loss: 0.7251\n",
            "INFO:root:loss: 0.7135\n",
            "loss: 0.7135\n",
            "INFO:root:loss: 0.7041\n",
            "loss: 0.7041\n",
            "INFO:root:loss: 0.6951\n",
            "loss: 0.6951\n",
            "INFO:root:loss: 0.6855\n",
            "loss: 0.6855\n",
            "INFO:root:loss: 0.6741\n",
            "loss: 0.6741\n",
            "INFO:root:loss: 0.6640\n",
            "loss: 0.6640\n",
            "INFO:root:loss: 0.6587\n",
            "loss: 0.6587\n",
            "INFO:root:loss: 0.6541\n",
            "loss: 0.6541\n",
            "INFO:root:loss: 0.6508\n",
            "loss: 0.6508\n",
            "INFO:root:loss: 0.6455\n",
            "loss: 0.6455\n",
            "INFO:root:loss: 0.6423\n",
            "loss: 0.6423\n",
            "INFO:root:loss: 0.6390\n",
            "loss: 0.6390\n",
            "INFO:root:loss: 0.6368\n",
            "loss: 0.6368\n",
            "INFO:root:loss: 0.6329\n",
            "loss: 0.6329\n",
            "INFO:root:loss: 0.6246\n",
            "loss: 0.6246\n",
            "INFO:root:loss: 0.6206\n",
            "loss: 0.6206\n",
            "INFO:root:loss: 0.6153\n",
            "loss: 0.6153\n",
            "INFO:root:loss: 0.6116\n",
            "loss: 0.6116\n",
            "INFO:root:loss: 0.6043\n",
            "loss: 0.6043\n",
            "INFO:root:loss: 0.6037\n",
            "loss: 0.6037\n",
            "INFO:root:loss: 0.6040\n",
            "loss: 0.6040\n",
            "INFO:root:loss: 0.6032\n",
            "loss: 0.6032\n",
            "INFO:root:loss: 0.5978\n",
            "loss: 0.5978\n",
            "INFO:root:loss: 0.5924\n",
            "loss: 0.5924\n",
            "INFO:root:loss: 0.5897\n",
            "loss: 0.5897\n",
            "INFO:root:loss: 0.5857\n",
            "loss: 0.5857\n",
            "INFO:root:loss: 0.5832\n",
            "loss: 0.5832\n",
            "INFO:root:loss: 0.5800\n",
            "loss: 0.5800\n",
            "INFO:root:loss: 0.5764\n",
            "loss: 0.5764\n",
            "INFO:root:loss: 0.5751\n",
            "loss: 0.5751\n",
            "INFO:root:loss: 0.5726\n",
            "loss: 0.5726\n",
            "INFO:root:loss: 0.5689\n",
            "loss: 0.5689\n",
            "INFO:root:loss: 0.5646\n",
            "loss: 0.5646\n",
            "INFO:root:loss: 0.5598\n",
            "loss: 0.5598\n",
            "INFO:root:loss: 0.5550\n",
            "loss: 0.5550\n",
            "INFO:root:loss: 0.5500\n",
            "loss: 0.5500\n",
            "INFO:root:loss: 0.5459\n",
            "loss: 0.5459\n",
            "INFO:root:loss: 0.5436\n",
            "loss: 0.5436\n",
            "INFO:root:loss: 0.5408\n",
            "loss: 0.5408\n",
            "INFO:root:loss: 0.5377\n",
            "loss: 0.5377\n",
            "INFO:root:loss: 0.5338\n",
            "loss: 0.5338\n",
            "INFO:root:loss: 0.5311\n",
            "loss: 0.5311\n",
            "INFO:root:loss: 0.5297\n",
            "loss: 0.5297\n",
            "INFO:root:loss: 0.5274\n",
            "loss: 0.5274\n",
            "INFO:root:loss: 0.5243\n",
            "loss: 0.5243\n",
            "INFO:root:loss: 0.5222\n",
            "loss: 0.5222\n",
            "INFO:root:loss: 0.5203\n",
            "loss: 0.5203\n",
            "INFO:root:loss: 0.5189\n",
            "loss: 0.5189\n",
            "INFO:root:loss: 0.5158\n",
            "loss: 0.5158\n",
            "INFO:root:loss: 0.5132\n",
            "loss: 0.5132\n",
            "INFO:root:loss: 0.5127\n",
            "loss: 0.5127\n",
            "INFO:root:loss: 0.5104\n",
            "loss: 0.5104\n",
            "INFO:root:loss: 0.5089\n",
            "loss: 0.5089\n",
            "INFO:root:loss: 0.5077\n",
            "loss: 0.5077\n",
            "INFO:root:loss: 0.5055\n",
            "loss: 0.5055\n",
            "INFO:root:loss: 0.5039\n",
            "loss: 0.5039\n",
            "INFO:root:>> saved: trained_models/RE_BERT_CV1_iob_epoch_1.model\n",
            ">> saved: trained_models/RE_BERT_CV1_iob_epoch_1.model\n"
          ]
        }
      ],
      "source": [
        "!python train.py --dataset CV1 --train_file train_iob_Evernote.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0piYeHiPbmfr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c9b70a7a-b608-4273-dec2-0de998a64776"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/Models/RE_BERT_CV1_iob_epoch_1.model'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "shutil.copy(\"trained_models/RE_BERT_CV1_iob_epoch_1.model\",\"/content/drive/MyDrive/Colab Notebooks/Models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iteration #3"
      ],
      "metadata": {
        "id": "3KKblLZjY3H1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SGJnnI9bpCb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ea2dbb9-447c-4d1b-f717-6c53bb5471b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "INFO:transformers.configuration_utils:Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "INFO:root:cuda memory allocated: 455604736\n",
            "cuda memory allocated: 455604736\n",
            "INFO:root:> n_trainable_params: 113617923, n_nontrainable_params: 0\n",
            "> n_trainable_params: 113617923, n_nontrainable_params: 0\n",
            "INFO:root:> training arguments:\n",
            "> training arguments:\n",
            "INFO:root:>>> model_name: RE_BERT\n",
            ">>> model_name: RE_BERT\n",
            "INFO:root:>>> dataset: CV2\n",
            ">>> dataset: CV2\n",
            "INFO:root:>>> train_file: train_iob_Facebook.txt\n",
            ">>> train_file: train_iob_Facebook.txt\n",
            "INFO:root:>>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            "INFO:root:>>> initializer: <function xavier_uniform_ at 0x7fb66d843170>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7fb66d843170>\n",
            "INFO:root:>>> lr: 2e-05\n",
            ">>> lr: 2e-05\n",
            "INFO:root:>>> dropout: 0.1\n",
            ">>> dropout: 0.1\n",
            "INFO:root:>>> l2reg: 0.01\n",
            ">>> l2reg: 0.01\n",
            "INFO:root:>>> num_epoch: 1\n",
            ">>> num_epoch: 1\n",
            "INFO:root:>>> batch_size: 16\n",
            ">>> batch_size: 16\n",
            "INFO:root:>>> log_step: 10\n",
            ">>> log_step: 10\n",
            "INFO:root:>>> bert_dim: 768\n",
            ">>> bert_dim: 768\n",
            "INFO:root:>>> pretrained_bert_name: bert-base-uncased\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            "INFO:root:>>> max_seq_len: 80\n",
            ">>> max_seq_len: 80\n",
            "INFO:root:>>> requirement_dim: 3\n",
            ">>> requirement_dim: 3\n",
            "INFO:root:>>> hops: 3\n",
            ">>> hops: 3\n",
            "INFO:root:>>> patience: 5\n",
            ">>> patience: 5\n",
            "INFO:root:>>> device: cuda\n",
            ">>> device: cuda\n",
            "INFO:root:>>> seed: None\n",
            ">>> seed: None\n",
            "INFO:root:>>> valset_ratio: 0\n",
            ">>> valset_ratio: 0\n",
            "INFO:root:>>> local_context_focus: cdm\n",
            ">>> local_context_focus: cdm\n",
            "INFO:root:>>> alpha: 3\n",
            ">>> alpha: 3\n",
            "INFO:root:>>> SRD: 3\n",
            ">>> SRD: 3\n",
            "INFO:root:>>> model_class: <class 'models.re_bert.RE_BERT'>\n",
            ">>> model_class: <class 'models.re_bert.RE_BERT'>\n",
            "INFO:root:>>> dataset_file: {'train': 'train_iob_Facebook.txt'}\n",
            ">>> dataset_file: {'train': 'train_iob_Facebook.txt'}\n",
            "INFO:root:>>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices', 'text_bert_indices', 'aspect_bert_indices']\n",
            ">>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices', 'text_bert_indices', 'aspect_bert_indices']\n",
            "INFO:root:>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "INFO:root:epoch: 1\n",
            "epoch: 1\n",
            "INFO:root:loss: 1.0470\n",
            "loss: 1.0470\n",
            "INFO:root:loss: 0.8543\n",
            "loss: 0.8543\n",
            "INFO:root:loss: 0.8845\n",
            "loss: 0.8845\n",
            "INFO:root:loss: 0.8384\n",
            "loss: 0.8384\n",
            "INFO:root:loss: 0.8333\n",
            "loss: 0.8333\n",
            "INFO:root:loss: 0.8135\n",
            "loss: 0.8135\n",
            "INFO:root:loss: 0.7882\n",
            "loss: 0.7882\n",
            "INFO:root:loss: 0.7827\n",
            "loss: 0.7827\n",
            "INFO:root:loss: 0.7790\n",
            "loss: 0.7790\n",
            "INFO:root:loss: 0.7718\n",
            "loss: 0.7718\n",
            "INFO:root:loss: 0.7671\n",
            "loss: 0.7671\n",
            "INFO:root:loss: 0.7655\n",
            "loss: 0.7655\n",
            "INFO:root:loss: 0.7718\n",
            "loss: 0.7718\n",
            "INFO:root:loss: 0.7707\n",
            "loss: 0.7707\n",
            "INFO:root:loss: 0.7651\n",
            "loss: 0.7651\n",
            "INFO:root:loss: 0.7569\n",
            "loss: 0.7569\n",
            "INFO:root:loss: 0.7495\n",
            "loss: 0.7495\n",
            "INFO:root:loss: 0.7468\n",
            "loss: 0.7468\n",
            "INFO:root:loss: 0.7351\n",
            "loss: 0.7351\n",
            "INFO:root:loss: 0.7214\n",
            "loss: 0.7214\n",
            "INFO:root:loss: 0.7115\n",
            "loss: 0.7115\n",
            "INFO:root:loss: 0.7028\n",
            "loss: 0.7028\n",
            "INFO:root:loss: 0.7012\n",
            "loss: 0.7012\n",
            "INFO:root:loss: 0.6933\n",
            "loss: 0.6933\n",
            "INFO:root:loss: 0.6924\n",
            "loss: 0.6924\n",
            "INFO:root:loss: 0.6868\n",
            "loss: 0.6868\n",
            "INFO:root:loss: 0.6803\n",
            "loss: 0.6803\n",
            "INFO:root:loss: 0.6764\n",
            "loss: 0.6764\n",
            "INFO:root:loss: 0.6708\n",
            "loss: 0.6708\n",
            "INFO:root:loss: 0.6657\n",
            "loss: 0.6657\n",
            "INFO:root:loss: 0.6600\n",
            "loss: 0.6600\n",
            "INFO:root:loss: 0.6554\n",
            "loss: 0.6554\n",
            "INFO:root:loss: 0.6475\n",
            "loss: 0.6475\n",
            "INFO:root:loss: 0.6408\n",
            "loss: 0.6408\n",
            "INFO:root:loss: 0.6345\n",
            "loss: 0.6345\n",
            "INFO:root:loss: 0.6321\n",
            "loss: 0.6321\n",
            "INFO:root:loss: 0.6270\n",
            "loss: 0.6270\n",
            "INFO:root:loss: 0.6222\n",
            "loss: 0.6222\n",
            "INFO:root:loss: 0.6173\n",
            "loss: 0.6173\n",
            "INFO:root:loss: 0.6125\n",
            "loss: 0.6125\n",
            "INFO:root:loss: 0.6098\n",
            "loss: 0.6098\n",
            "INFO:root:loss: 0.6057\n",
            "loss: 0.6057\n",
            "INFO:root:loss: 0.6025\n",
            "loss: 0.6025\n",
            "INFO:root:loss: 0.5996\n",
            "loss: 0.5996\n",
            "INFO:root:loss: 0.5940\n",
            "loss: 0.5940\n",
            "INFO:root:loss: 0.5900\n",
            "loss: 0.5900\n",
            "INFO:root:loss: 0.5847\n",
            "loss: 0.5847\n",
            "INFO:root:loss: 0.5791\n",
            "loss: 0.5791\n",
            "INFO:root:loss: 0.5748\n",
            "loss: 0.5748\n",
            "INFO:root:loss: 0.5714\n",
            "loss: 0.5714\n",
            "INFO:root:loss: 0.5722\n",
            "loss: 0.5722\n",
            "INFO:root:loss: 0.5708\n",
            "loss: 0.5708\n",
            "INFO:root:loss: 0.5669\n",
            "loss: 0.5669\n",
            "INFO:root:loss: 0.5632\n",
            "loss: 0.5632\n",
            "INFO:root:loss: 0.5587\n",
            "loss: 0.5587\n",
            "INFO:root:loss: 0.5536\n",
            "loss: 0.5536\n",
            "INFO:root:loss: 0.5491\n",
            "loss: 0.5491\n",
            "INFO:root:loss: 0.5468\n",
            "loss: 0.5468\n",
            "INFO:root:loss: 0.5442\n",
            "loss: 0.5442\n",
            "INFO:root:loss: 0.5420\n",
            "loss: 0.5420\n",
            "INFO:root:loss: 0.5387\n",
            "loss: 0.5387\n",
            "INFO:root:loss: 0.5365\n",
            "loss: 0.5365\n",
            "INFO:root:loss: 0.5334\n",
            "loss: 0.5334\n",
            "INFO:root:loss: 0.5300\n",
            "loss: 0.5300\n",
            "INFO:root:loss: 0.5279\n",
            "loss: 0.5279\n",
            "INFO:root:loss: 0.5251\n",
            "loss: 0.5251\n",
            "INFO:root:loss: 0.5225\n",
            "loss: 0.5225\n",
            "INFO:root:loss: 0.5190\n",
            "loss: 0.5190\n",
            "INFO:root:loss: 0.5183\n",
            "loss: 0.5183\n",
            "INFO:root:loss: 0.5144\n",
            "loss: 0.5144\n",
            "INFO:root:loss: 0.5119\n",
            "loss: 0.5119\n",
            "INFO:root:loss: 0.5110\n",
            "loss: 0.5110\n",
            "INFO:root:loss: 0.5098\n",
            "loss: 0.5098\n",
            "INFO:root:loss: 0.5076\n",
            "loss: 0.5076\n",
            "INFO:root:loss: 0.5052\n",
            "loss: 0.5052\n",
            "INFO:root:loss: 0.5036\n",
            "loss: 0.5036\n",
            "INFO:root:loss: 0.5005\n",
            "loss: 0.5005\n",
            "INFO:root:loss: 0.4986\n",
            "loss: 0.4986\n",
            "INFO:root:loss: 0.4973\n",
            "loss: 0.4973\n",
            "INFO:root:loss: 0.4962\n",
            "loss: 0.4962\n",
            "INFO:root:loss: 0.4941\n",
            "loss: 0.4941\n",
            "INFO:root:loss: 0.4927\n",
            "loss: 0.4927\n",
            "INFO:root:>> saved: trained_models/RE_BERT_CV2_iob_epoch_1.model\n",
            ">> saved: trained_models/RE_BERT_CV2_iob_epoch_1.model\n"
          ]
        }
      ],
      "source": [
        "!python train.py --dataset CV2 --train_file train_iob_Facebook.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "As6GIW52bt_f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6700fbdc-4da1-4704-c523-78d79378eb04"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/Models/RE_BERT_CV2_iob_epoch_1.model'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "shutil.copy(\"trained_models/RE_BERT_CV2_iob_epoch_1.model\",\"/content/drive/MyDrive/Colab Notebooks/Models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iteration #4"
      ],
      "metadata": {
        "id": "G3BuqGD6Y630"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRpfysF7bvJW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef3800b4-c8e7-4b79-e814-44c653f9619d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "INFO:transformers.configuration_utils:Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "INFO:root:cuda memory allocated: 455604736\n",
            "cuda memory allocated: 455604736\n",
            "INFO:root:> n_trainable_params: 113617923, n_nontrainable_params: 0\n",
            "> n_trainable_params: 113617923, n_nontrainable_params: 0\n",
            "INFO:root:> training arguments:\n",
            "> training arguments:\n",
            "INFO:root:>>> model_name: RE_BERT\n",
            ">>> model_name: RE_BERT\n",
            "INFO:root:>>> dataset: CV3\n",
            ">>> dataset: CV3\n",
            "INFO:root:>>> train_file: train_iob_Netflix.txt\n",
            ">>> train_file: train_iob_Netflix.txt\n",
            "INFO:root:>>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            "INFO:root:>>> initializer: <function xavier_uniform_ at 0x7f52a63f1170>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f52a63f1170>\n",
            "INFO:root:>>> lr: 2e-05\n",
            ">>> lr: 2e-05\n",
            "INFO:root:>>> dropout: 0.1\n",
            ">>> dropout: 0.1\n",
            "INFO:root:>>> l2reg: 0.01\n",
            ">>> l2reg: 0.01\n",
            "INFO:root:>>> num_epoch: 1\n",
            ">>> num_epoch: 1\n",
            "INFO:root:>>> batch_size: 16\n",
            ">>> batch_size: 16\n",
            "INFO:root:>>> log_step: 10\n",
            ">>> log_step: 10\n",
            "INFO:root:>>> bert_dim: 768\n",
            ">>> bert_dim: 768\n",
            "INFO:root:>>> pretrained_bert_name: bert-base-uncased\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            "INFO:root:>>> max_seq_len: 80\n",
            ">>> max_seq_len: 80\n",
            "INFO:root:>>> requirement_dim: 3\n",
            ">>> requirement_dim: 3\n",
            "INFO:root:>>> hops: 3\n",
            ">>> hops: 3\n",
            "INFO:root:>>> patience: 5\n",
            ">>> patience: 5\n",
            "INFO:root:>>> device: cuda\n",
            ">>> device: cuda\n",
            "INFO:root:>>> seed: None\n",
            ">>> seed: None\n",
            "INFO:root:>>> valset_ratio: 0\n",
            ">>> valset_ratio: 0\n",
            "INFO:root:>>> local_context_focus: cdm\n",
            ">>> local_context_focus: cdm\n",
            "INFO:root:>>> alpha: 3\n",
            ">>> alpha: 3\n",
            "INFO:root:>>> SRD: 3\n",
            ">>> SRD: 3\n",
            "INFO:root:>>> model_class: <class 'models.re_bert.RE_BERT'>\n",
            ">>> model_class: <class 'models.re_bert.RE_BERT'>\n",
            "INFO:root:>>> dataset_file: {'train': 'train_iob_Netflix.txt'}\n",
            ">>> dataset_file: {'train': 'train_iob_Netflix.txt'}\n",
            "INFO:root:>>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices', 'text_bert_indices', 'aspect_bert_indices']\n",
            ">>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices', 'text_bert_indices', 'aspect_bert_indices']\n",
            "INFO:root:>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "INFO:root:epoch: 1\n",
            "epoch: 1\n",
            "INFO:root:loss: 0.8389\n",
            "loss: 0.8389\n",
            "INFO:root:loss: 0.8426\n",
            "loss: 0.8426\n",
            "INFO:root:loss: 0.8421\n",
            "loss: 0.8421\n",
            "INFO:root:loss: 0.7994\n",
            "loss: 0.7994\n",
            "INFO:root:loss: 0.7846\n",
            "loss: 0.7846\n",
            "INFO:root:loss: 0.7850\n",
            "loss: 0.7850\n",
            "INFO:root:loss: 0.7792\n",
            "loss: 0.7792\n",
            "INFO:root:loss: 0.7723\n",
            "loss: 0.7723\n",
            "INFO:root:loss: 0.7639\n",
            "loss: 0.7639\n",
            "INFO:root:loss: 0.7558\n",
            "loss: 0.7558\n",
            "INFO:root:loss: 0.7340\n",
            "loss: 0.7340\n",
            "INFO:root:loss: 0.7180\n",
            "loss: 0.7180\n",
            "INFO:root:loss: 0.7143\n",
            "loss: 0.7143\n",
            "INFO:root:loss: 0.6940\n",
            "loss: 0.6940\n",
            "INFO:root:loss: 0.6830\n",
            "loss: 0.6830\n",
            "INFO:root:loss: 0.6773\n",
            "loss: 0.6773\n",
            "INFO:root:loss: 0.6749\n",
            "loss: 0.6749\n",
            "INFO:root:loss: 0.6724\n",
            "loss: 0.6724\n",
            "INFO:root:loss: 0.6672\n",
            "loss: 0.6672\n",
            "INFO:root:loss: 0.6635\n",
            "loss: 0.6635\n",
            "INFO:root:loss: 0.6632\n",
            "loss: 0.6632\n",
            "INFO:root:loss: 0.6543\n",
            "loss: 0.6543\n",
            "INFO:root:loss: 0.6505\n",
            "loss: 0.6505\n",
            "INFO:root:loss: 0.6434\n",
            "loss: 0.6434\n",
            "INFO:root:loss: 0.6355\n",
            "loss: 0.6355\n",
            "INFO:root:loss: 0.6282\n",
            "loss: 0.6282\n",
            "INFO:root:loss: 0.6239\n",
            "loss: 0.6239\n",
            "INFO:root:loss: 0.6201\n",
            "loss: 0.6201\n",
            "INFO:root:loss: 0.6124\n",
            "loss: 0.6124\n",
            "INFO:root:loss: 0.6072\n",
            "loss: 0.6072\n",
            "INFO:root:loss: 0.6041\n",
            "loss: 0.6041\n",
            "INFO:root:loss: 0.5968\n",
            "loss: 0.5968\n",
            "INFO:root:loss: 0.5942\n",
            "loss: 0.5942\n",
            "INFO:root:loss: 0.5892\n",
            "loss: 0.5892\n",
            "INFO:root:loss: 0.5822\n",
            "loss: 0.5822\n",
            "INFO:root:loss: 0.5779\n",
            "loss: 0.5779\n",
            "INFO:root:loss: 0.5776\n",
            "loss: 0.5776\n",
            "INFO:root:loss: 0.5758\n",
            "loss: 0.5758\n",
            "INFO:root:loss: 0.5711\n",
            "loss: 0.5711\n",
            "INFO:root:loss: 0.5686\n",
            "loss: 0.5686\n",
            "INFO:root:loss: 0.5663\n",
            "loss: 0.5663\n",
            "INFO:root:loss: 0.5643\n",
            "loss: 0.5643\n",
            "INFO:root:loss: 0.5592\n",
            "loss: 0.5592\n",
            "INFO:root:loss: 0.5583\n",
            "loss: 0.5583\n",
            "INFO:root:loss: 0.5569\n",
            "loss: 0.5569\n",
            "INFO:root:loss: 0.5540\n",
            "loss: 0.5540\n",
            "INFO:root:loss: 0.5509\n",
            "loss: 0.5509\n",
            "INFO:root:loss: 0.5485\n",
            "loss: 0.5485\n",
            "INFO:root:loss: 0.5473\n",
            "loss: 0.5473\n",
            "INFO:root:loss: 0.5464\n",
            "loss: 0.5464\n",
            "INFO:root:loss: 0.5427\n",
            "loss: 0.5427\n",
            "INFO:root:loss: 0.5407\n",
            "loss: 0.5407\n",
            "INFO:root:loss: 0.5378\n",
            "loss: 0.5378\n",
            "INFO:root:loss: 0.5360\n",
            "loss: 0.5360\n",
            "INFO:root:loss: 0.5319\n",
            "loss: 0.5319\n",
            "INFO:root:loss: 0.5276\n",
            "loss: 0.5276\n",
            "INFO:root:loss: 0.5245\n",
            "loss: 0.5245\n",
            "INFO:root:loss: 0.5219\n",
            "loss: 0.5219\n",
            "INFO:root:loss: 0.5197\n",
            "loss: 0.5197\n",
            "INFO:root:loss: 0.5172\n",
            "loss: 0.5172\n",
            "INFO:root:loss: 0.5149\n",
            "loss: 0.5149\n",
            "INFO:root:loss: 0.5145\n",
            "loss: 0.5145\n",
            "INFO:root:loss: 0.5126\n",
            "loss: 0.5126\n",
            "INFO:root:loss: 0.5125\n",
            "loss: 0.5125\n",
            "INFO:root:loss: 0.5100\n",
            "loss: 0.5100\n",
            "INFO:root:loss: 0.5072\n",
            "loss: 0.5072\n",
            "INFO:root:loss: 0.5040\n",
            "loss: 0.5040\n",
            "INFO:root:loss: 0.5011\n",
            "loss: 0.5011\n",
            "INFO:root:loss: 0.4985\n",
            "loss: 0.4985\n",
            "INFO:root:loss: 0.4961\n",
            "loss: 0.4961\n",
            "INFO:root:loss: 0.4943\n",
            "loss: 0.4943\n",
            "INFO:root:loss: 0.4919\n",
            "loss: 0.4919\n",
            "INFO:root:loss: 0.4895\n",
            "loss: 0.4895\n",
            "INFO:root:loss: 0.4870\n",
            "loss: 0.4870\n",
            "INFO:root:loss: 0.4850\n",
            "loss: 0.4850\n",
            "INFO:root:loss: 0.4838\n",
            "loss: 0.4838\n",
            "INFO:root:loss: 0.4817\n",
            "loss: 0.4817\n",
            "INFO:root:>> saved: trained_models/RE_BERT_CV3_iob_epoch_1.model\n",
            ">> saved: trained_models/RE_BERT_CV3_iob_epoch_1.model\n"
          ]
        }
      ],
      "source": [
        "!python train.py --dataset CV3 --train_file train_iob_Netflix.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7enhRsAXbxVy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8b6b725c-7baa-4757-cb9a-7f20c398bc93"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/Models/RE_BERT_CV3_iob_epoch_1.model'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "shutil.copy(\"trained_models/RE_BERT_CV3_iob_epoch_1.model\",\"/content/drive/MyDrive/Colab Notebooks/Models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iteration #5"
      ],
      "metadata": {
        "id": "zsqE-ZwGY_xg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2L6idJYfb6K4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "935a6a32-55b0-4aad-8d0f-614d875cab55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "INFO:transformers.configuration_utils:Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "INFO:root:cuda memory allocated: 455604736\n",
            "cuda memory allocated: 455604736\n",
            "INFO:root:> n_trainable_params: 113617923, n_nontrainable_params: 0\n",
            "> n_trainable_params: 113617923, n_nontrainable_params: 0\n",
            "INFO:root:> training arguments:\n",
            "> training arguments:\n",
            "INFO:root:>>> model_name: RE_BERT\n",
            ">>> model_name: RE_BERT\n",
            "INFO:root:>>> dataset: CV4\n",
            ">>> dataset: CV4\n",
            "INFO:root:>>> train_file: train_iob_PhotoEditor.txt\n",
            ">>> train_file: train_iob_PhotoEditor.txt\n",
            "INFO:root:>>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            "INFO:root:>>> initializer: <function xavier_uniform_ at 0x7f1b84abf170>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f1b84abf170>\n",
            "INFO:root:>>> lr: 2e-05\n",
            ">>> lr: 2e-05\n",
            "INFO:root:>>> dropout: 0.1\n",
            ">>> dropout: 0.1\n",
            "INFO:root:>>> l2reg: 0.01\n",
            ">>> l2reg: 0.01\n",
            "INFO:root:>>> num_epoch: 1\n",
            ">>> num_epoch: 1\n",
            "INFO:root:>>> batch_size: 16\n",
            ">>> batch_size: 16\n",
            "INFO:root:>>> log_step: 10\n",
            ">>> log_step: 10\n",
            "INFO:root:>>> bert_dim: 768\n",
            ">>> bert_dim: 768\n",
            "INFO:root:>>> pretrained_bert_name: bert-base-uncased\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            "INFO:root:>>> max_seq_len: 80\n",
            ">>> max_seq_len: 80\n",
            "INFO:root:>>> requirement_dim: 3\n",
            ">>> requirement_dim: 3\n",
            "INFO:root:>>> hops: 3\n",
            ">>> hops: 3\n",
            "INFO:root:>>> patience: 5\n",
            ">>> patience: 5\n",
            "INFO:root:>>> device: cuda\n",
            ">>> device: cuda\n",
            "INFO:root:>>> seed: None\n",
            ">>> seed: None\n",
            "INFO:root:>>> valset_ratio: 0\n",
            ">>> valset_ratio: 0\n",
            "INFO:root:>>> local_context_focus: cdm\n",
            ">>> local_context_focus: cdm\n",
            "INFO:root:>>> alpha: 3\n",
            ">>> alpha: 3\n",
            "INFO:root:>>> SRD: 3\n",
            ">>> SRD: 3\n",
            "INFO:root:>>> model_class: <class 'models.re_bert.RE_BERT'>\n",
            ">>> model_class: <class 'models.re_bert.RE_BERT'>\n",
            "INFO:root:>>> dataset_file: {'train': 'train_iob_PhotoEditor.txt'}\n",
            ">>> dataset_file: {'train': 'train_iob_PhotoEditor.txt'}\n",
            "INFO:root:>>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices', 'text_bert_indices', 'aspect_bert_indices']\n",
            ">>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices', 'text_bert_indices', 'aspect_bert_indices']\n",
            "INFO:root:>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "INFO:root:epoch: 1\n",
            "epoch: 1\n",
            "INFO:root:loss: 0.8356\n",
            "loss: 0.8356\n",
            "INFO:root:loss: 0.8730\n",
            "loss: 0.8730\n",
            "INFO:root:loss: 0.8294\n",
            "loss: 0.8294\n",
            "INFO:root:loss: 0.8090\n",
            "loss: 0.8090\n",
            "INFO:root:loss: 0.8030\n",
            "loss: 0.8030\n",
            "INFO:root:loss: 0.7936\n",
            "loss: 0.7936\n",
            "INFO:root:loss: 0.7873\n",
            "loss: 0.7873\n",
            "INFO:root:loss: 0.7882\n",
            "loss: 0.7882\n",
            "INFO:root:loss: 0.7660\n",
            "loss: 0.7660\n",
            "INFO:root:loss: 0.7612\n",
            "loss: 0.7612\n",
            "INFO:root:loss: 0.7514\n",
            "loss: 0.7514\n",
            "INFO:root:loss: 0.7408\n",
            "loss: 0.7408\n",
            "INFO:root:loss: 0.7304\n",
            "loss: 0.7304\n",
            "INFO:root:loss: 0.7156\n",
            "loss: 0.7156\n",
            "INFO:root:loss: 0.7069\n",
            "loss: 0.7069\n",
            "INFO:root:loss: 0.7034\n",
            "loss: 0.7034\n",
            "INFO:root:loss: 0.6928\n",
            "loss: 0.6928\n",
            "INFO:root:loss: 0.6925\n",
            "loss: 0.6925\n",
            "INFO:root:loss: 0.6770\n",
            "loss: 0.6770\n",
            "INFO:root:loss: 0.6670\n",
            "loss: 0.6670\n",
            "INFO:root:loss: 0.6638\n",
            "loss: 0.6638\n",
            "INFO:root:loss: 0.6593\n",
            "loss: 0.6593\n",
            "INFO:root:loss: 0.6561\n",
            "loss: 0.6561\n",
            "INFO:root:loss: 0.6522\n",
            "loss: 0.6522\n",
            "INFO:root:loss: 0.6517\n",
            "loss: 0.6517\n",
            "INFO:root:loss: 0.6486\n",
            "loss: 0.6486\n",
            "INFO:root:loss: 0.6441\n",
            "loss: 0.6441\n",
            "INFO:root:loss: 0.6369\n",
            "loss: 0.6369\n",
            "INFO:root:loss: 0.6313\n",
            "loss: 0.6313\n",
            "INFO:root:loss: 0.6246\n",
            "loss: 0.6246\n",
            "INFO:root:loss: 0.6176\n",
            "loss: 0.6176\n",
            "INFO:root:loss: 0.6125\n",
            "loss: 0.6125\n",
            "INFO:root:loss: 0.6091\n",
            "loss: 0.6091\n",
            "INFO:root:loss: 0.6053\n",
            "loss: 0.6053\n",
            "INFO:root:loss: 0.6005\n",
            "loss: 0.6005\n",
            "INFO:root:loss: 0.5970\n",
            "loss: 0.5970\n",
            "INFO:root:loss: 0.5908\n",
            "loss: 0.5908\n",
            "INFO:root:loss: 0.5847\n",
            "loss: 0.5847\n",
            "INFO:root:loss: 0.5796\n",
            "loss: 0.5796\n",
            "INFO:root:loss: 0.5752\n",
            "loss: 0.5752\n",
            "INFO:root:loss: 0.5722\n",
            "loss: 0.5722\n",
            "INFO:root:loss: 0.5669\n",
            "loss: 0.5669\n",
            "INFO:root:loss: 0.5626\n",
            "loss: 0.5626\n",
            "INFO:root:loss: 0.5589\n",
            "loss: 0.5589\n",
            "INFO:root:loss: 0.5549\n",
            "loss: 0.5549\n",
            "INFO:root:loss: 0.5539\n",
            "loss: 0.5539\n",
            "INFO:root:loss: 0.5512\n",
            "loss: 0.5512\n",
            "INFO:root:loss: 0.5478\n",
            "loss: 0.5478\n",
            "INFO:root:loss: 0.5433\n",
            "loss: 0.5433\n",
            "INFO:root:loss: 0.5392\n",
            "loss: 0.5392\n",
            "INFO:root:loss: 0.5384\n",
            "loss: 0.5384\n",
            "INFO:root:loss: 0.5349\n",
            "loss: 0.5349\n",
            "INFO:root:loss: 0.5329\n",
            "loss: 0.5329\n",
            "INFO:root:loss: 0.5316\n",
            "loss: 0.5316\n",
            "INFO:root:loss: 0.5291\n",
            "loss: 0.5291\n",
            "INFO:root:loss: 0.5271\n",
            "loss: 0.5271\n",
            "INFO:root:loss: 0.5226\n",
            "loss: 0.5226\n",
            "INFO:root:loss: 0.5195\n",
            "loss: 0.5195\n",
            "INFO:root:loss: 0.5165\n",
            "loss: 0.5165\n",
            "INFO:root:loss: 0.5137\n",
            "loss: 0.5137\n",
            "INFO:root:loss: 0.5090\n",
            "loss: 0.5090\n",
            "INFO:root:loss: 0.5039\n",
            "loss: 0.5039\n",
            "INFO:root:loss: 0.5022\n",
            "loss: 0.5022\n",
            "INFO:root:loss: 0.4990\n",
            "loss: 0.4990\n",
            "INFO:root:loss: 0.4962\n",
            "loss: 0.4962\n",
            "INFO:root:loss: 0.4937\n",
            "loss: 0.4937\n",
            "INFO:root:loss: 0.4920\n",
            "loss: 0.4920\n",
            "INFO:root:loss: 0.4903\n",
            "loss: 0.4903\n",
            "INFO:root:loss: 0.4883\n",
            "loss: 0.4883\n",
            "INFO:root:loss: 0.4872\n",
            "loss: 0.4872\n",
            "INFO:root:loss: 0.4862\n",
            "loss: 0.4862\n",
            "INFO:root:loss: 0.4862\n",
            "loss: 0.4862\n",
            "INFO:root:loss: 0.4837\n",
            "loss: 0.4837\n",
            "INFO:root:loss: 0.4821\n",
            "loss: 0.4821\n",
            "INFO:root:loss: 0.4802\n",
            "loss: 0.4802\n",
            "INFO:root:loss: 0.4787\n",
            "loss: 0.4787\n",
            "INFO:root:loss: 0.4777\n",
            "loss: 0.4777\n",
            "INFO:root:loss: 0.4771\n",
            "loss: 0.4771\n",
            "INFO:root:loss: 0.4764\n",
            "loss: 0.4764\n",
            "INFO:root:loss: 0.4744\n",
            "loss: 0.4744\n",
            "INFO:root:loss: 0.4724\n",
            "loss: 0.4724\n",
            "INFO:root:loss: 0.4703\n",
            "loss: 0.4703\n",
            "INFO:root:loss: 0.4693\n",
            "loss: 0.4693\n",
            "INFO:root:loss: 0.4670\n",
            "loss: 0.4670\n",
            "INFO:root:loss: 0.4663\n",
            "loss: 0.4663\n",
            "INFO:root:loss: 0.4645\n",
            "loss: 0.4645\n",
            "INFO:root:loss: 0.4626\n",
            "loss: 0.4626\n",
            "INFO:root:loss: 0.4614\n",
            "loss: 0.4614\n",
            "INFO:root:loss: 0.4594\n",
            "loss: 0.4594\n",
            "INFO:root:loss: 0.4587\n",
            "loss: 0.4587\n",
            "INFO:root:loss: 0.4570\n",
            "loss: 0.4570\n",
            "INFO:root:>> saved: trained_models/RE_BERT_CV4_iob_epoch_1.model\n",
            ">> saved: trained_models/RE_BERT_CV4_iob_epoch_1.model\n"
          ]
        }
      ],
      "source": [
        "!python train.py --dataset CV4 --train_file train_iob_PhotoEditor.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjFUwFSabzah",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d69ae6e4-2e06-4ab9-e763-b1ca1d4aef0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/Models/RE_BERT_CV4_iob_epoch_1.model'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "shutil.copy(\"trained_models/RE_BERT_CV4_iob_epoch_1.model\",\"/content/drive/MyDrive/Colab Notebooks/Models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iteration #6"
      ],
      "metadata": {
        "id": "VVt5xZj36_M2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1itHeaKbb7jL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "241fa406-08cd-42fa-bc42-414c29506eb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "INFO:transformers.configuration_utils:Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "INFO:root:cuda memory allocated: 455604736\n",
            "cuda memory allocated: 455604736\n",
            "INFO:root:> n_trainable_params: 113617923, n_nontrainable_params: 0\n",
            "> n_trainable_params: 113617923, n_nontrainable_params: 0\n",
            "INFO:root:> training arguments:\n",
            "> training arguments:\n",
            "INFO:root:>>> model_name: RE_BERT\n",
            ">>> model_name: RE_BERT\n",
            "INFO:root:>>> dataset: CV5\n",
            ">>> dataset: CV5\n",
            "INFO:root:>>> train_file: train_iob_Spotify.txt\n",
            ">>> train_file: train_iob_Spotify.txt\n",
            "INFO:root:>>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            "INFO:root:>>> initializer: <function xavier_uniform_ at 0x7fe77bb91170>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7fe77bb91170>\n",
            "INFO:root:>>> lr: 2e-05\n",
            ">>> lr: 2e-05\n",
            "INFO:root:>>> dropout: 0.1\n",
            ">>> dropout: 0.1\n",
            "INFO:root:>>> l2reg: 0.01\n",
            ">>> l2reg: 0.01\n",
            "INFO:root:>>> num_epoch: 1\n",
            ">>> num_epoch: 1\n",
            "INFO:root:>>> batch_size: 16\n",
            ">>> batch_size: 16\n",
            "INFO:root:>>> log_step: 10\n",
            ">>> log_step: 10\n",
            "INFO:root:>>> bert_dim: 768\n",
            ">>> bert_dim: 768\n",
            "INFO:root:>>> pretrained_bert_name: bert-base-uncased\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            "INFO:root:>>> max_seq_len: 80\n",
            ">>> max_seq_len: 80\n",
            "INFO:root:>>> requirement_dim: 3\n",
            ">>> requirement_dim: 3\n",
            "INFO:root:>>> hops: 3\n",
            ">>> hops: 3\n",
            "INFO:root:>>> patience: 5\n",
            ">>> patience: 5\n",
            "INFO:root:>>> device: cuda\n",
            ">>> device: cuda\n",
            "INFO:root:>>> seed: None\n",
            ">>> seed: None\n",
            "INFO:root:>>> valset_ratio: 0\n",
            ">>> valset_ratio: 0\n",
            "INFO:root:>>> local_context_focus: cdm\n",
            ">>> local_context_focus: cdm\n",
            "INFO:root:>>> alpha: 3\n",
            ">>> alpha: 3\n",
            "INFO:root:>>> SRD: 3\n",
            ">>> SRD: 3\n",
            "INFO:root:>>> model_class: <class 'models.re_bert.RE_BERT'>\n",
            ">>> model_class: <class 'models.re_bert.RE_BERT'>\n",
            "INFO:root:>>> dataset_file: {'train': 'train_iob_Spotify.txt'}\n",
            ">>> dataset_file: {'train': 'train_iob_Spotify.txt'}\n",
            "INFO:root:>>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices', 'text_bert_indices', 'aspect_bert_indices']\n",
            ">>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices', 'text_bert_indices', 'aspect_bert_indices']\n",
            "INFO:root:>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "INFO:root:epoch: 1\n",
            "epoch: 1\n",
            "INFO:root:loss: 0.9935\n",
            "loss: 0.9935\n",
            "INFO:root:loss: 0.9049\n",
            "loss: 0.9049\n",
            "INFO:root:loss: 0.8953\n",
            "loss: 0.8953\n",
            "INFO:root:loss: 0.8678\n",
            "loss: 0.8678\n",
            "INFO:root:loss: 0.8582\n",
            "loss: 0.8582\n",
            "INFO:root:loss: 0.8390\n",
            "loss: 0.8390\n",
            "INFO:root:loss: 0.8160\n",
            "loss: 0.8160\n",
            "INFO:root:loss: 0.8007\n",
            "loss: 0.8007\n",
            "INFO:root:loss: 0.7878\n",
            "loss: 0.7878\n",
            "INFO:root:loss: 0.7805\n",
            "loss: 0.7805\n",
            "INFO:root:loss: 0.7667\n",
            "loss: 0.7667\n",
            "INFO:root:loss: 0.7543\n",
            "loss: 0.7543\n",
            "INFO:root:loss: 0.7504\n",
            "loss: 0.7504\n",
            "INFO:root:loss: 0.7402\n",
            "loss: 0.7402\n",
            "INFO:root:loss: 0.7280\n",
            "loss: 0.7280\n",
            "INFO:root:loss: 0.7175\n",
            "loss: 0.7175\n",
            "INFO:root:loss: 0.7172\n",
            "loss: 0.7172\n",
            "INFO:root:loss: 0.7043\n",
            "loss: 0.7043\n",
            "INFO:root:loss: 0.6935\n",
            "loss: 0.6935\n",
            "INFO:root:loss: 0.6858\n",
            "loss: 0.6858\n",
            "INFO:root:loss: 0.6779\n",
            "loss: 0.6779\n",
            "INFO:root:loss: 0.6736\n",
            "loss: 0.6736\n",
            "INFO:root:loss: 0.6675\n",
            "loss: 0.6675\n",
            "INFO:root:loss: 0.6653\n",
            "loss: 0.6653\n",
            "INFO:root:loss: 0.6580\n",
            "loss: 0.6580\n",
            "INFO:root:loss: 0.6493\n",
            "loss: 0.6493\n",
            "INFO:root:loss: 0.6383\n",
            "loss: 0.6383\n",
            "INFO:root:loss: 0.6348\n",
            "loss: 0.6348\n",
            "INFO:root:loss: 0.6266\n",
            "loss: 0.6266\n",
            "INFO:root:loss: 0.6188\n",
            "loss: 0.6188\n",
            "INFO:root:loss: 0.6110\n",
            "loss: 0.6110\n",
            "INFO:root:loss: 0.6056\n",
            "loss: 0.6056\n",
            "INFO:root:loss: 0.5994\n",
            "loss: 0.5994\n",
            "INFO:root:loss: 0.5977\n",
            "loss: 0.5977\n",
            "INFO:root:loss: 0.5940\n",
            "loss: 0.5940\n",
            "INFO:root:loss: 0.5917\n",
            "loss: 0.5917\n",
            "INFO:root:loss: 0.5915\n",
            "loss: 0.5915\n",
            "INFO:root:loss: 0.5912\n",
            "loss: 0.5912\n",
            "INFO:root:loss: 0.5875\n",
            "loss: 0.5875\n",
            "INFO:root:loss: 0.5813\n",
            "loss: 0.5813\n",
            "INFO:root:loss: 0.5800\n",
            "loss: 0.5800\n",
            "INFO:root:loss: 0.5778\n",
            "loss: 0.5778\n",
            "INFO:root:loss: 0.5750\n",
            "loss: 0.5750\n",
            "INFO:root:loss: 0.5710\n",
            "loss: 0.5710\n",
            "INFO:root:loss: 0.5690\n",
            "loss: 0.5690\n",
            "INFO:root:loss: 0.5652\n",
            "loss: 0.5652\n",
            "INFO:root:loss: 0.5634\n",
            "loss: 0.5634\n",
            "INFO:root:loss: 0.5613\n",
            "loss: 0.5613\n",
            "INFO:root:loss: 0.5587\n",
            "loss: 0.5587\n",
            "INFO:root:loss: 0.5562\n",
            "loss: 0.5562\n",
            "INFO:root:loss: 0.5565\n",
            "loss: 0.5565\n",
            "INFO:root:loss: 0.5538\n",
            "loss: 0.5538\n",
            "INFO:root:loss: 0.5506\n",
            "loss: 0.5506\n",
            "INFO:root:loss: 0.5469\n",
            "loss: 0.5469\n",
            "INFO:root:loss: 0.5460\n",
            "loss: 0.5460\n",
            "INFO:root:loss: 0.5426\n",
            "loss: 0.5426\n",
            "INFO:root:loss: 0.5399\n",
            "loss: 0.5399\n",
            "INFO:root:loss: 0.5387\n",
            "loss: 0.5387\n",
            "INFO:root:loss: 0.5372\n",
            "loss: 0.5372\n",
            "INFO:root:loss: 0.5362\n",
            "loss: 0.5362\n",
            "INFO:root:loss: 0.5366\n",
            "loss: 0.5366\n",
            "INFO:root:loss: 0.5356\n",
            "loss: 0.5356\n",
            "INFO:root:loss: 0.5340\n",
            "loss: 0.5340\n",
            "INFO:root:loss: 0.5315\n",
            "loss: 0.5315\n",
            "INFO:root:loss: 0.5288\n",
            "loss: 0.5288\n",
            "INFO:root:loss: 0.5273\n",
            "loss: 0.5273\n",
            "INFO:root:loss: 0.5242\n",
            "loss: 0.5242\n",
            "INFO:root:loss: 0.5220\n",
            "loss: 0.5220\n",
            "INFO:root:loss: 0.5211\n",
            "loss: 0.5211\n",
            "INFO:root:loss: 0.5180\n",
            "loss: 0.5180\n",
            "INFO:root:loss: 0.5152\n",
            "loss: 0.5152\n",
            "INFO:root:loss: 0.5133\n",
            "loss: 0.5133\n",
            "INFO:root:loss: 0.5095\n",
            "loss: 0.5095\n",
            "INFO:root:loss: 0.5088\n",
            "loss: 0.5088\n",
            "INFO:root:loss: 0.5069\n",
            "loss: 0.5069\n",
            "INFO:root:loss: 0.5052\n",
            "loss: 0.5052\n",
            "INFO:root:loss: 0.5029\n",
            "loss: 0.5029\n",
            "INFO:root:loss: 0.5012\n",
            "loss: 0.5012\n",
            "INFO:root:loss: 0.4981\n",
            "loss: 0.4981\n",
            "INFO:root:loss: 0.4961\n",
            "loss: 0.4961\n",
            "INFO:root:loss: 0.4948\n",
            "loss: 0.4948\n",
            "INFO:root:loss: 0.4933\n",
            "loss: 0.4933\n",
            "INFO:root:loss: 0.4920\n",
            "loss: 0.4920\n",
            "INFO:root:loss: 0.4907\n",
            "loss: 0.4907\n",
            "INFO:root:loss: 0.4887\n",
            "loss: 0.4887\n",
            "INFO:root:>> saved: trained_models/RE_BERT_CV5_iob_epoch_1.model\n",
            ">> saved: trained_models/RE_BERT_CV5_iob_epoch_1.model\n"
          ]
        }
      ],
      "source": [
        "!python train.py --dataset CV5 --train_file train_iob_Spotify.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhqCSwUQb0iU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7bb84849-dd68-4bf3-dcc6-25dc86875e80"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/Models/RE_BERT_CV5_iob_epoch_1.model'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "shutil.copy(\"trained_models/RE_BERT_CV5_iob_epoch_1.model\",\"/content/drive/MyDrive/Colab Notebooks/Models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iteration #7"
      ],
      "metadata": {
        "id": "eR5ARhLS7BQG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLJMHNiyb9iq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb31432d-dd5a-417d-c3ae-ecc2e972ee14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "INFO:transformers.configuration_utils:Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "INFO:root:cuda memory allocated: 455604736\n",
            "cuda memory allocated: 455604736\n",
            "INFO:root:> n_trainable_params: 113617923, n_nontrainable_params: 0\n",
            "> n_trainable_params: 113617923, n_nontrainable_params: 0\n",
            "INFO:root:> training arguments:\n",
            "> training arguments:\n",
            "INFO:root:>>> model_name: RE_BERT\n",
            ">>> model_name: RE_BERT\n",
            "INFO:root:>>> dataset: CV6\n",
            ">>> dataset: CV6\n",
            "INFO:root:>>> train_file: train_iob_Twitter.txt\n",
            ">>> train_file: train_iob_Twitter.txt\n",
            "INFO:root:>>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            "INFO:root:>>> initializer: <function xavier_uniform_ at 0x7f58307a8170>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f58307a8170>\n",
            "INFO:root:>>> lr: 2e-05\n",
            ">>> lr: 2e-05\n",
            "INFO:root:>>> dropout: 0.1\n",
            ">>> dropout: 0.1\n",
            "INFO:root:>>> l2reg: 0.01\n",
            ">>> l2reg: 0.01\n",
            "INFO:root:>>> num_epoch: 1\n",
            ">>> num_epoch: 1\n",
            "INFO:root:>>> batch_size: 16\n",
            ">>> batch_size: 16\n",
            "INFO:root:>>> log_step: 10\n",
            ">>> log_step: 10\n",
            "INFO:root:>>> bert_dim: 768\n",
            ">>> bert_dim: 768\n",
            "INFO:root:>>> pretrained_bert_name: bert-base-uncased\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            "INFO:root:>>> max_seq_len: 80\n",
            ">>> max_seq_len: 80\n",
            "INFO:root:>>> requirement_dim: 3\n",
            ">>> requirement_dim: 3\n",
            "INFO:root:>>> hops: 3\n",
            ">>> hops: 3\n",
            "INFO:root:>>> patience: 5\n",
            ">>> patience: 5\n",
            "INFO:root:>>> device: cuda\n",
            ">>> device: cuda\n",
            "INFO:root:>>> seed: None\n",
            ">>> seed: None\n",
            "INFO:root:>>> valset_ratio: 0\n",
            ">>> valset_ratio: 0\n",
            "INFO:root:>>> local_context_focus: cdm\n",
            ">>> local_context_focus: cdm\n",
            "INFO:root:>>> alpha: 3\n",
            ">>> alpha: 3\n",
            "INFO:root:>>> SRD: 3\n",
            ">>> SRD: 3\n",
            "INFO:root:>>> model_class: <class 'models.re_bert.RE_BERT'>\n",
            ">>> model_class: <class 'models.re_bert.RE_BERT'>\n",
            "INFO:root:>>> dataset_file: {'train': 'train_iob_Twitter.txt'}\n",
            ">>> dataset_file: {'train': 'train_iob_Twitter.txt'}\n",
            "INFO:root:>>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices', 'text_bert_indices', 'aspect_bert_indices']\n",
            ">>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices', 'text_bert_indices', 'aspect_bert_indices']\n",
            "INFO:root:>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "INFO:root:epoch: 1\n",
            "epoch: 1\n",
            "INFO:root:loss: 0.9777\n",
            "loss: 0.9777\n",
            "INFO:root:loss: 0.9022\n",
            "loss: 0.9022\n",
            "INFO:root:loss: 0.8706\n",
            "loss: 0.8706\n",
            "INFO:root:loss: 0.8579\n",
            "loss: 0.8579\n",
            "INFO:root:loss: 0.8062\n",
            "loss: 0.8062\n",
            "INFO:root:loss: 0.8138\n",
            "loss: 0.8138\n",
            "INFO:root:loss: 0.8076\n",
            "loss: 0.8076\n",
            "INFO:root:loss: 0.8048\n",
            "loss: 0.8048\n",
            "INFO:root:loss: 0.7908\n",
            "loss: 0.7908\n",
            "INFO:root:loss: 0.7903\n",
            "loss: 0.7903\n",
            "INFO:root:loss: 0.7823\n",
            "loss: 0.7823\n",
            "INFO:root:loss: 0.7851\n",
            "loss: 0.7851\n",
            "INFO:root:loss: 0.7760\n",
            "loss: 0.7760\n",
            "INFO:root:loss: 0.7771\n",
            "loss: 0.7771\n",
            "INFO:root:loss: 0.7702\n",
            "loss: 0.7702\n",
            "INFO:root:loss: 0.7611\n",
            "loss: 0.7611\n",
            "INFO:root:loss: 0.7604\n",
            "loss: 0.7604\n",
            "INFO:root:loss: 0.7512\n",
            "loss: 0.7512\n",
            "INFO:root:loss: 0.7466\n",
            "loss: 0.7466\n",
            "INFO:root:loss: 0.7363\n",
            "loss: 0.7363\n",
            "INFO:root:loss: 0.7252\n",
            "loss: 0.7252\n",
            "INFO:root:loss: 0.7138\n",
            "loss: 0.7138\n",
            "INFO:root:loss: 0.7086\n",
            "loss: 0.7086\n",
            "INFO:root:loss: 0.7025\n",
            "loss: 0.7025\n",
            "INFO:root:loss: 0.6912\n",
            "loss: 0.6912\n",
            "INFO:root:loss: 0.6857\n",
            "loss: 0.6857\n",
            "INFO:root:loss: 0.6863\n",
            "loss: 0.6863\n",
            "INFO:root:loss: 0.6830\n",
            "loss: 0.6830\n",
            "INFO:root:loss: 0.6789\n",
            "loss: 0.6789\n",
            "INFO:root:loss: 0.6728\n",
            "loss: 0.6728\n",
            "INFO:root:loss: 0.6667\n",
            "loss: 0.6667\n",
            "INFO:root:loss: 0.6607\n",
            "loss: 0.6607\n",
            "INFO:root:loss: 0.6525\n",
            "loss: 0.6525\n",
            "INFO:root:loss: 0.6485\n",
            "loss: 0.6485\n",
            "INFO:root:loss: 0.6464\n",
            "loss: 0.6464\n",
            "INFO:root:loss: 0.6411\n",
            "loss: 0.6411\n",
            "INFO:root:loss: 0.6350\n",
            "loss: 0.6350\n",
            "INFO:root:loss: 0.6310\n",
            "loss: 0.6310\n",
            "INFO:root:loss: 0.6256\n",
            "loss: 0.6256\n",
            "INFO:root:loss: 0.6205\n",
            "loss: 0.6205\n",
            "INFO:root:loss: 0.6193\n",
            "loss: 0.6193\n",
            "INFO:root:loss: 0.6163\n",
            "loss: 0.6163\n",
            "INFO:root:loss: 0.6105\n",
            "loss: 0.6105\n",
            "INFO:root:loss: 0.6078\n",
            "loss: 0.6078\n",
            "INFO:root:loss: 0.6063\n",
            "loss: 0.6063\n",
            "INFO:root:loss: 0.6023\n",
            "loss: 0.6023\n",
            "INFO:root:loss: 0.5975\n",
            "loss: 0.5975\n",
            "INFO:root:loss: 0.5929\n",
            "loss: 0.5929\n",
            "INFO:root:loss: 0.5867\n",
            "loss: 0.5867\n",
            "INFO:root:loss: 0.5832\n",
            "loss: 0.5832\n",
            "INFO:root:loss: 0.5811\n",
            "loss: 0.5811\n",
            "INFO:root:loss: 0.5769\n",
            "loss: 0.5769\n",
            "INFO:root:loss: 0.5726\n",
            "loss: 0.5726\n",
            "INFO:root:loss: 0.5670\n",
            "loss: 0.5670\n",
            "INFO:root:loss: 0.5647\n",
            "loss: 0.5647\n",
            "INFO:root:loss: 0.5603\n",
            "loss: 0.5603\n",
            "INFO:root:loss: 0.5579\n",
            "loss: 0.5579\n",
            "INFO:root:loss: 0.5553\n",
            "loss: 0.5553\n",
            "INFO:root:loss: 0.5526\n",
            "loss: 0.5526\n",
            "INFO:root:loss: 0.5489\n",
            "loss: 0.5489\n",
            "INFO:root:loss: 0.5456\n",
            "loss: 0.5456\n",
            "INFO:root:loss: 0.5441\n",
            "loss: 0.5441\n",
            "INFO:root:loss: 0.5401\n",
            "loss: 0.5401\n",
            "INFO:root:loss: 0.5382\n",
            "loss: 0.5382\n",
            "INFO:root:loss: 0.5363\n",
            "loss: 0.5363\n",
            "INFO:root:loss: 0.5334\n",
            "loss: 0.5334\n",
            "INFO:root:loss: 0.5303\n",
            "loss: 0.5303\n",
            "INFO:root:loss: 0.5288\n",
            "loss: 0.5288\n",
            "INFO:root:loss: 0.5262\n",
            "loss: 0.5262\n",
            "INFO:root:loss: 0.5247\n",
            "loss: 0.5247\n",
            "INFO:root:loss: 0.5224\n",
            "loss: 0.5224\n",
            "INFO:root:loss: 0.5214\n",
            "loss: 0.5214\n",
            "INFO:root:loss: 0.5197\n",
            "loss: 0.5197\n",
            "INFO:root:loss: 0.5177\n",
            "loss: 0.5177\n",
            "INFO:root:loss: 0.5149\n",
            "loss: 0.5149\n",
            "INFO:root:loss: 0.5136\n",
            "loss: 0.5136\n",
            "INFO:root:loss: 0.5113\n",
            "loss: 0.5113\n",
            "INFO:root:loss: 0.5102\n",
            "loss: 0.5102\n",
            "INFO:root:loss: 0.5086\n",
            "loss: 0.5086\n",
            "INFO:root:loss: 0.5064\n",
            "loss: 0.5064\n",
            "INFO:root:loss: 0.5059\n",
            "loss: 0.5059\n",
            "INFO:root:loss: 0.5039\n",
            "loss: 0.5039\n",
            "INFO:root:loss: 0.5011\n",
            "loss: 0.5011\n",
            "INFO:root:loss: 0.4997\n",
            "loss: 0.4997\n",
            "INFO:root:loss: 0.4991\n",
            "loss: 0.4991\n",
            "INFO:root:loss: 0.4989\n",
            "loss: 0.4989\n",
            "INFO:root:loss: 0.4979\n",
            "loss: 0.4979\n",
            "INFO:root:loss: 0.4957\n",
            "loss: 0.4957\n",
            "INFO:root:loss: 0.4937\n",
            "loss: 0.4937\n",
            "INFO:root:>> saved: trained_models/RE_BERT_CV6_iob_epoch_1.model\n",
            ">> saved: trained_models/RE_BERT_CV6_iob_epoch_1.model\n"
          ]
        }
      ],
      "source": [
        "!python train.py --dataset CV6 --train_file train_iob_Twitter.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRqXzCJJb14-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0fdce57d-8f48-4124-ecb8-436eab409819"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/Models/RE_BERT_CV6_iob_epoch_1.model'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "shutil.copy(\"trained_models/RE_BERT_CV6_iob_epoch_1.model\",\"/content/drive/MyDrive/Colab Notebooks/Models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iteration #8"
      ],
      "metadata": {
        "id": "jViAVgNz-Go1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArFu1Bh7b_VO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6d5b1cb-051a-4b53-b8f6-4f7ac4a5fce0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "INFO:transformers.configuration_utils:Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "INFO:root:cuda memory allocated: 455604736\n",
            "cuda memory allocated: 455604736\n",
            "INFO:root:> n_trainable_params: 113617923, n_nontrainable_params: 0\n",
            "> n_trainable_params: 113617923, n_nontrainable_params: 0\n",
            "INFO:root:> training arguments:\n",
            "> training arguments:\n",
            "INFO:root:>>> model_name: RE_BERT\n",
            ">>> model_name: RE_BERT\n",
            "INFO:root:>>> dataset: CV7\n",
            ">>> dataset: CV7\n",
            "INFO:root:>>> train_file: train_iob_WhatsApp.txt\n",
            ">>> train_file: train_iob_WhatsApp.txt\n",
            "INFO:root:>>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            "INFO:root:>>> initializer: <function xavier_uniform_ at 0x7fca8e5a8170>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7fca8e5a8170>\n",
            "INFO:root:>>> lr: 2e-05\n",
            ">>> lr: 2e-05\n",
            "INFO:root:>>> dropout: 0.1\n",
            ">>> dropout: 0.1\n",
            "INFO:root:>>> l2reg: 0.01\n",
            ">>> l2reg: 0.01\n",
            "INFO:root:>>> num_epoch: 1\n",
            ">>> num_epoch: 1\n",
            "INFO:root:>>> batch_size: 16\n",
            ">>> batch_size: 16\n",
            "INFO:root:>>> log_step: 10\n",
            ">>> log_step: 10\n",
            "INFO:root:>>> bert_dim: 768\n",
            ">>> bert_dim: 768\n",
            "INFO:root:>>> pretrained_bert_name: bert-base-uncased\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            "INFO:root:>>> max_seq_len: 80\n",
            ">>> max_seq_len: 80\n",
            "INFO:root:>>> requirement_dim: 3\n",
            ">>> requirement_dim: 3\n",
            "INFO:root:>>> hops: 3\n",
            ">>> hops: 3\n",
            "INFO:root:>>> patience: 5\n",
            ">>> patience: 5\n",
            "INFO:root:>>> device: cuda\n",
            ">>> device: cuda\n",
            "INFO:root:>>> seed: None\n",
            ">>> seed: None\n",
            "INFO:root:>>> valset_ratio: 0\n",
            ">>> valset_ratio: 0\n",
            "INFO:root:>>> local_context_focus: cdm\n",
            ">>> local_context_focus: cdm\n",
            "INFO:root:>>> alpha: 3\n",
            ">>> alpha: 3\n",
            "INFO:root:>>> SRD: 3\n",
            ">>> SRD: 3\n",
            "INFO:root:>>> model_class: <class 'models.re_bert.RE_BERT'>\n",
            ">>> model_class: <class 'models.re_bert.RE_BERT'>\n",
            "INFO:root:>>> dataset_file: {'train': 'train_iob_WhatsApp.txt'}\n",
            ">>> dataset_file: {'train': 'train_iob_WhatsApp.txt'}\n",
            "INFO:root:>>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices', 'text_bert_indices', 'aspect_bert_indices']\n",
            ">>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices', 'text_bert_indices', 'aspect_bert_indices']\n",
            "INFO:root:>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "INFO:root:epoch: 1\n",
            "epoch: 1\n",
            "INFO:root:loss: 0.9314\n",
            "loss: 0.9314\n",
            "INFO:root:loss: 0.8584\n",
            "loss: 0.8584\n",
            "INFO:root:loss: 0.8332\n",
            "loss: 0.8332\n",
            "INFO:root:loss: 0.7925\n",
            "loss: 0.7925\n",
            "INFO:root:loss: 0.7929\n",
            "loss: 0.7929\n",
            "INFO:root:loss: 0.8062\n",
            "loss: 0.8062\n",
            "INFO:root:loss: 0.7902\n",
            "loss: 0.7902\n",
            "INFO:root:loss: 0.7868\n",
            "loss: 0.7868\n",
            "INFO:root:loss: 0.7810\n",
            "loss: 0.7810\n",
            "INFO:root:loss: 0.7742\n",
            "loss: 0.7742\n",
            "INFO:root:loss: 0.7726\n",
            "loss: 0.7726\n",
            "INFO:root:loss: 0.7734\n",
            "loss: 0.7734\n",
            "INFO:root:loss: 0.7740\n",
            "loss: 0.7740\n",
            "INFO:root:loss: 0.7732\n",
            "loss: 0.7732\n",
            "INFO:root:loss: 0.7710\n",
            "loss: 0.7710\n",
            "INFO:root:loss: 0.7652\n",
            "loss: 0.7652\n",
            "INFO:root:loss: 0.7573\n",
            "loss: 0.7573\n",
            "INFO:root:loss: 0.7482\n",
            "loss: 0.7482\n",
            "INFO:root:loss: 0.7418\n",
            "loss: 0.7418\n",
            "INFO:root:loss: 0.7304\n",
            "loss: 0.7304\n",
            "INFO:root:loss: 0.7178\n",
            "loss: 0.7178\n",
            "INFO:root:loss: 0.7050\n",
            "loss: 0.7050\n",
            "INFO:root:loss: 0.6921\n",
            "loss: 0.6921\n",
            "INFO:root:loss: 0.6840\n",
            "loss: 0.6840\n",
            "INFO:root:loss: 0.6827\n",
            "loss: 0.6827\n",
            "INFO:root:loss: 0.6769\n",
            "loss: 0.6769\n",
            "INFO:root:loss: 0.6714\n",
            "loss: 0.6714\n",
            "INFO:root:loss: 0.6648\n",
            "loss: 0.6648\n",
            "INFO:root:loss: 0.6567\n",
            "loss: 0.6567\n",
            "INFO:root:loss: 0.6505\n",
            "loss: 0.6505\n",
            "INFO:root:loss: 0.6417\n",
            "loss: 0.6417\n",
            "INFO:root:loss: 0.6336\n",
            "loss: 0.6336\n",
            "INFO:root:loss: 0.6303\n",
            "loss: 0.6303\n",
            "INFO:root:loss: 0.6237\n",
            "loss: 0.6237\n",
            "INFO:root:loss: 0.6186\n",
            "loss: 0.6186\n",
            "INFO:root:loss: 0.6112\n",
            "loss: 0.6112\n",
            "INFO:root:loss: 0.6061\n",
            "loss: 0.6061\n",
            "INFO:root:loss: 0.6011\n",
            "loss: 0.6011\n",
            "INFO:root:loss: 0.5944\n",
            "loss: 0.5944\n",
            "INFO:root:loss: 0.5901\n",
            "loss: 0.5901\n",
            "INFO:root:loss: 0.5862\n",
            "loss: 0.5862\n",
            "INFO:root:loss: 0.5819\n",
            "loss: 0.5819\n",
            "INFO:root:loss: 0.5770\n",
            "loss: 0.5770\n",
            "INFO:root:loss: 0.5715\n",
            "loss: 0.5715\n",
            "INFO:root:loss: 0.5650\n",
            "loss: 0.5650\n",
            "INFO:root:loss: 0.5602\n",
            "loss: 0.5602\n",
            "INFO:root:loss: 0.5597\n",
            "loss: 0.5597\n",
            "INFO:root:loss: 0.5563\n",
            "loss: 0.5563\n",
            "INFO:root:loss: 0.5563\n",
            "loss: 0.5563\n",
            "INFO:root:loss: 0.5530\n",
            "loss: 0.5530\n",
            "INFO:root:loss: 0.5483\n",
            "loss: 0.5483\n",
            "INFO:root:loss: 0.5447\n",
            "loss: 0.5447\n",
            "INFO:root:loss: 0.5424\n",
            "loss: 0.5424\n",
            "INFO:root:loss: 0.5386\n",
            "loss: 0.5386\n",
            "INFO:root:loss: 0.5370\n",
            "loss: 0.5370\n",
            "INFO:root:loss: 0.5334\n",
            "loss: 0.5334\n",
            "INFO:root:loss: 0.5316\n",
            "loss: 0.5316\n",
            "INFO:root:loss: 0.5276\n",
            "loss: 0.5276\n",
            "INFO:root:loss: 0.5240\n",
            "loss: 0.5240\n",
            "INFO:root:loss: 0.5227\n",
            "loss: 0.5227\n",
            "INFO:root:loss: 0.5197\n",
            "loss: 0.5197\n",
            "INFO:root:loss: 0.5161\n",
            "loss: 0.5161\n",
            "INFO:root:loss: 0.5127\n",
            "loss: 0.5127\n",
            "INFO:root:loss: 0.5098\n",
            "loss: 0.5098\n",
            "INFO:root:loss: 0.5064\n",
            "loss: 0.5064\n",
            "INFO:root:loss: 0.5046\n",
            "loss: 0.5046\n",
            "INFO:root:loss: 0.5019\n",
            "loss: 0.5019\n",
            "INFO:root:loss: 0.4998\n",
            "loss: 0.4998\n",
            "INFO:root:loss: 0.4977\n",
            "loss: 0.4977\n",
            "INFO:root:loss: 0.4960\n",
            "loss: 0.4960\n",
            "INFO:root:loss: 0.4951\n",
            "loss: 0.4951\n",
            "INFO:root:loss: 0.4933\n",
            "loss: 0.4933\n",
            "INFO:root:loss: 0.4909\n",
            "loss: 0.4909\n",
            "INFO:root:loss: 0.4878\n",
            "loss: 0.4878\n",
            "INFO:root:loss: 0.4842\n",
            "loss: 0.4842\n",
            "INFO:root:loss: 0.4807\n",
            "loss: 0.4807\n",
            "INFO:root:loss: 0.4779\n",
            "loss: 0.4779\n",
            "INFO:root:loss: 0.4747\n",
            "loss: 0.4747\n",
            "INFO:root:loss: 0.4732\n",
            "loss: 0.4732\n",
            "INFO:root:loss: 0.4717\n",
            "loss: 0.4717\n",
            "INFO:root:loss: 0.4697\n",
            "loss: 0.4697\n",
            "INFO:root:loss: 0.4665\n",
            "loss: 0.4665\n",
            "INFO:root:loss: 0.4649\n",
            "loss: 0.4649\n",
            "INFO:root:loss: 0.4634\n",
            "loss: 0.4634\n",
            "INFO:root:loss: 0.4615\n",
            "loss: 0.4615\n",
            "INFO:root:loss: 0.4602\n",
            "loss: 0.4602\n",
            "INFO:root:loss: 0.4578\n",
            "loss: 0.4578\n",
            "INFO:root:loss: 0.4551\n",
            "loss: 0.4551\n",
            "INFO:root:loss: 0.4539\n",
            "loss: 0.4539\n",
            "INFO:root:loss: 0.4524\n",
            "loss: 0.4524\n",
            "INFO:root:>> saved: trained_models/RE_BERT_CV7_iob_epoch_1.model\n",
            ">> saved: trained_models/RE_BERT_CV7_iob_epoch_1.model\n"
          ]
        }
      ],
      "source": [
        "!python train.py --dataset CV7 --train_file train_iob_WhatsApp.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOBN0VQzb23w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6bafcd62-4abc-4079-8ec0-52c90dd1617d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/Models/RE_BERT_CV7_iob_epoch_1.model'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "shutil.copy(\"trained_models/RE_BERT_CV7_iob_epoch_1.model\",\"/content/drive/MyDrive/Colab Notebooks/Models\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}